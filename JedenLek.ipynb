{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JedenLek.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1q9ZTDRvP34L3rGffrwmaSDp6KC6yAssQ",
      "authorship_tag": "ABX9TyPCheIDgii9HpBfEwcWWDES",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EwaKiz/GraphNeuralNetworks/blob/main/JedenLek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LJn_-kal7htT",
        "cellView": "form",
        "outputId": "c5917d24-e0a6-4ccd-9f41-92263cef3d42"
      },
      "source": [
        "#@title install torch\n",
        "# Enforce pytorch version 1.6.0\n",
        "# import torch\n",
        "# if torch.__version__ != '1.6.0':\n",
        "!pip uninstall torch -y\n",
        "!pip uninstall torchvision -y\n",
        "#   !pip install torch==1.6.0\n",
        "#   !pip install torchvision==0.7.0\n",
        "!pip install torch==1.8.0 torchvision  -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
        "# Check pytorch version and make sure you use a GPU Kernel\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!python --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.10.0+cu111\n",
            "Uninstalling torch-1.10.0+cu111:\n",
            "  Successfully uninstalled torch-1.10.0+cu111\n",
            "Found existing installation: torchvision 0.11.1+cu111\n",
            "Uninstalling torchvision-0.11.1+cu111:\n",
            "  Successfully uninstalled torchvision-0.11.1+cu111\n",
            "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
            "Collecting torch==1.8.0\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 2.4 MB/s eta 0:07:58tcmalloc: large alloc 1147494400 bytes == 0x557f6d86e000 @  0x7fd85f0ed615 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336abd00 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3363a039 0x557f3367d409 0x557f33638c52 0x557f336abc25 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a7915 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 11.8 MB/s eta 0:01:19tcmalloc: large alloc 1434370048 bytes == 0x557fb1ec4000 @  0x7fd85f0ed615 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336abd00 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3363a039 0x557f3367d409 0x557f33638c52 0x557f336abc25 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a7915 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 27.8 MB/s eta 0:00:24tcmalloc: large alloc 1792966656 bytes == 0x557f36cf6000 @  0x7fd85f0ed615 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336abd00 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3363a039 0x557f3367d409 0x557f33638c52 0x557f336abc25 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a7915 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.4 MB/s eta 0:03:24tcmalloc: large alloc 2241208320 bytes == 0x557fa1ade000 @  0x7fd85f0ed615 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336abd00 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3372ac66 0x557f336a7daf 0x557f3363a039 0x557f3367d409 0x557f33638c52 0x557f336abc25 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a7915 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.4 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x558027440000 @  0x7fd85f0ec1e7 0x557f3366b067 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x558111af8000 @  0x7fd85f0ed615 0x557f336354cc 0x557f3371547a 0x557f336382ed 0x557f33729e1d 0x557f336abe99 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a7c0d 0x557f33639afa 0x557f336a7c0d 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f33639bda 0x557f336a8737 0x557f336a69ee 0x557f3363a271\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 2.5 kB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.7.4.3)\n",
            "  Downloading torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 87 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (21.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.9 MB 49 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 59 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 27 kB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 63 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 1.5 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 117 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 60.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111 torchvision-0.9.0+cu111\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8.0+cu111\n",
            "11.1\n",
            "Python 3.7.12\n",
            "Wed Nov  3 11:01:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8JY6iem8NZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a55ccea-7964-4d82-c2fe-96d3c5afcc85"
      },
      "source": [
        "import torch\n",
        "# pytorch_version = f\"torch-{torch.__version__}+cu{torch.version.cuda.replace('.', '')}.html\"\n",
        "pytorch_version = 'torch-1.9.0+cu111.html'\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBsZNjjz8kAp"
      },
      "source": [
        "from torch_geometric.data import InMemoryDataset\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.data import Data"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k88PStaE8V5s"
      },
      "source": [
        "class CosmicGraph(InMemoryDataset):\n",
        "    def __init__(self, root,dataset,out_name, transform=None, pre_transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.out_name = out_name\n",
        "        super(CosmicGraph,self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        \n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.out_name]\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self):\n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "        # process by session_id\n",
        "        grouped = self.dataset.groupby('CosmicId')\n",
        "        for CosmicId, group in tqdm(grouped):\n",
        "            group = group.reset_index(drop=True)\n",
        "            node_features = group.loc[group.CosmicId==CosmicId,['gene_id','mutation_code_1','expression_cor','cancer census','MUT_TYPE_gain','MUT_TYPE_loss']].sort_values('gene_id').drop('gene_id',axis=1).values\n",
        "\n",
        "            node_features = torch.FloatTensor(node_features)\n",
        "            x = node_features\n",
        "\n",
        "            # y = torch.FloatTensor([group[\"AUC\"].values[0]])\n",
        "            y = torch.FloatTensor([[group[\"AUC\"].values[0]]])\n",
        "\n",
        "            data = Data(x=x, edge_index=edge_index, y=y)\n",
        "            data_list.append(data)\n",
        "        \n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NV7bIwV8SIY"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkpvAQ1S80yJ"
      },
      "source": [
        "CellLineData = pd.read_csv('/content/drive/MyDrive/ABT_final.csv')\n",
        "CellLineData=CellLineData[['GENE_SYMBOLS','CosmicId','mutation_code_1','0','cancer census','MUT_TYPE_gain','MUT_TYPE_loss']]\n",
        "GenesSl = pd.read_csv('/content/drive/MyDrive/B2_top500_CL_druggable_genes_final.csv',encoding='latin-1')\n",
        "Genes = list(CellLineData.GENE_SYMBOLS.unique())\n",
        "GenesSl = GenesSl.loc[(GenesSl['geneA'].isin(Genes)) & (GenesSl['geneB'].isin(Genes))]\n",
        "genes_encoder = LabelEncoder()\n",
        "CellLineData['gene_id'] = genes_encoder.fit_transform(CellLineData.GENE_SYMBOLS)\n",
        "GenesSl = pd.merge(GenesSl,CellLineData[['GENE_SYMBOLS','gene_id']].drop_duplicates(),left_on ='geneA',right_on='GENE_SYMBOLS',how='left').drop('GENE_SYMBOLS',axis=1)\n",
        "GenesSl.rename(columns={\"gene_id\":\"a_id\"},inplace=True)\n",
        "GenesSl = pd.merge(GenesSl,CellLineData[['GENE_SYMBOLS','gene_id']].drop_duplicates(),left_on ='geneB',right_on='GENE_SYMBOLS',how='left').drop('GENE_SYMBOLS',axis=1)\n",
        "GenesSl.rename(columns={\"gene_id\":\"b_id\"},inplace=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMAm1lsfCW6R"
      },
      "source": [
        "nodes_1 = np.append(GenesSl.a_id.values,GenesSl.b_id.values)\n",
        "nodes_2 = np.append(GenesSl.b_id.values,GenesSl.a_id.values)\n",
        "edge_index = torch.tensor([nodes_1, nodes_2], dtype=torch.long)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnZCp-KjCnUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "f380344c-235a-4321-82c1-47174303f787"
      },
      "source": [
        "CellLineData"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GENE_SYMBOLS</th>\n",
              "      <th>CosmicId</th>\n",
              "      <th>mutation_code_1</th>\n",
              "      <th>0</th>\n",
              "      <th>cancer census</th>\n",
              "      <th>MUT_TYPE_gain</th>\n",
              "      <th>MUT_TYPE_loss</th>\n",
              "      <th>gene_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABCB1</td>\n",
              "      <td>683665</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.869685</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABCB1</td>\n",
              "      <td>683667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.265524</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABCB1</td>\n",
              "      <td>684052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.758147</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABCB1</td>\n",
              "      <td>684055</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.089067</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABCB1</td>\n",
              "      <td>684057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.912594</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371565</th>\n",
              "      <td>ZNF451</td>\n",
              "      <td>1660036</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.627502</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371566</th>\n",
              "      <td>ZNF451</td>\n",
              "      <td>1674021</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.025006</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371567</th>\n",
              "      <td>ZNF451</td>\n",
              "      <td>1723793</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.240690</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371568</th>\n",
              "      <td>ZNF451</td>\n",
              "      <td>1723794</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.580327</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371569</th>\n",
              "      <td>ZNF451</td>\n",
              "      <td>11223344</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.815078</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>371570 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       GENE_SYMBOLS  CosmicId  ...  MUT_TYPE_loss  gene_id\n",
              "0             ABCB1    683665  ...            0.0        0\n",
              "1             ABCB1    683667  ...            0.0        0\n",
              "2             ABCB1    684052  ...            0.0        0\n",
              "3             ABCB1    684055  ...            0.0        0\n",
              "4             ABCB1    684057  ...            0.0        0\n",
              "...             ...       ...  ...            ...      ...\n",
              "371565       ZNF451   1660036  ...            0.0      364\n",
              "371566       ZNF451   1674021  ...            0.0      364\n",
              "371567       ZNF451   1723793  ...            0.0      364\n",
              "371568       ZNF451   1723794  ...            0.0      364\n",
              "371569       ZNF451  11223344  ...            0.0      364\n",
              "\n",
              "[371570 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz6kAU5eFkAI"
      },
      "source": [
        "response = pd.read_csv('/content/drive/MyDrive/drug_response_for_drug-cell_line_pairs.csv')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7e-m31SGQpj"
      },
      "source": [
        "'''Filter by drug id '''\n",
        "def FilterByDrug(dataset,drug_id):\n",
        "  dataset = dataset.loc[dataset['DRUG_ID']==drug_id]\n",
        "  return dataset"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LBikGSMB-M"
      },
      "source": [
        "response_Dabrafenib = FilterByDrug(response,1373)\n",
        "cell_line_data_resp = pd.merge(CellLineData,response_Dabrafenib[['COSMIC_ID','AUC']].drop_duplicates(),left_on ='CosmicId',right_on='COSMIC_ID',how='left').drop('COSMIC_ID',axis=1)\n",
        "cell_line_data_resp.dropna(subset = [\"AUC\"], inplace=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtgR7UVLNxb5"
      },
      "source": [
        "'split data into training and test set'\n",
        "cell_lines = list(cell_line_data_resp['CosmicId'].unique())\n",
        "random.seed(3)\n",
        "test = random.sample(cell_lines,100)\n",
        "val = random.sample([x for x in cell_lines if x not in test],100)\n",
        "train = [x for x in cell_lines if x not in test and x not in val]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ugDbUa7Q0dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5db1402-4a5f-4a54-aa83-d7d2e9c44f43"
      },
      "source": [
        "906851 in train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ9bo9ADLXBw"
      },
      "source": [
        "label=[]\n",
        "for i in range(len(cell_line_data_resp)):\n",
        "  row = cell_line_data_resp.iloc[i]\n",
        "  cl = row['CosmicId']\n",
        "  if cl in test:\n",
        "    label.append('test')\n",
        "  elif cl in val:\n",
        "    label.append('val')\n",
        "  else:\n",
        "    label.append('train')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ712mayL1KC"
      },
      "source": [
        "cell_line_data_resp['label'] = label"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgYZLI6SVP82"
      },
      "source": [
        "def NormalizeExpression(dataframe,mean=0,std =0):\n",
        "  genes = list(dataframe.GENE_SYMBOLS.unique())\n",
        "  i = 1\n",
        "  for gene in genes:\n",
        "    sample = dataframe.loc[dataframe['GENE_SYMBOLS']==gene]\n",
        "    train = sample.loc[sample['label']=='train']\n",
        "    mean = train['0'].mean()\n",
        "    std = train['0'].std()\n",
        "    sample['expression_cor'] = (sample['0']-mean)/std\n",
        "    if i == 1 :\n",
        "      result = sample\n",
        "    else:\n",
        "      result = pd.concat([result,sample])\n",
        "    i+=1\n",
        "  return result"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKZApxmzM8aR"
      },
      "source": [
        "normalized = NormalizeExpression(cell_line_data_resp)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM53YlufNJaS"
      },
      "source": [
        "Test_set = normalized.loc[normalized['CosmicId'].isin(test)]\n",
        "Train_set = normalized.loc[normalized['CosmicId'].isin(train)]\n",
        "Validation_set = normalized.loc[normalized['CosmicId'].isin(val)]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ezevWQ10pX"
      },
      "source": [
        "expression = Train_set.pivot_table('expression_cor', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('exp_')\n",
        "mutation = Train_set.pivot_table('mutation_code_1', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('mut_')\n",
        "gain = Train_set.pivot_table('MUT_TYPE_gain', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('gain_')\n",
        "loss = Train_set.pivot_table('MUT_TYPE_loss', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('loss_')\n",
        "vector_rep = pd.merge(expression,mutation,on = \"CosmicId\")\n",
        "vector_rep = pd.merge(vector_rep,gain,on = \"CosmicId\")\n",
        "vector_rep = pd.merge(vector_rep,loss,on = \"CosmicId\")\n",
        "vector_rep_train = pd.merge(response_Dabrafenib[['COSMIC_ID','AUC']].drop_duplicates(),vector_rep,right_on ='CosmicId',left_on='COSMIC_ID',how='left')\n",
        "vector_rep_train.dropna(subset = [\"exp_ABCB1\"], inplace=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZW4NwxP2XA9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "b945b0ac-b749-4ee1-ef66-6f82cea824f4"
      },
      "source": [
        "expression = Test_set.pivot_table('expression_cor', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('exp_')\n",
        "mutation = Test_set.pivot_table('mutation_code_1', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('mut_')\n",
        "gain = Test_set.pivot_table('MUT_TYPE_gain', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('gain_')\n",
        "loss = Test_set.pivot_table('MUT_TYPE_loss', ['CosmicId'], 'GENE_SYMBOLS').add_prefix('loss_')\n",
        "vector_rep = pd.merge(expression,mutation,on = \"CosmicId\")\n",
        "vector_rep = pd.merge(vector_rep,gain,on = \"CosmicId\")\n",
        "vector_rep = pd.merge(vector_rep,loss,on = \"CosmicId\")\n",
        "vector_rep_test = pd.merge(response_Dabrafenib[['COSMIC_ID','AUC']].drop_duplicates(),vector_rep,right_on ='CosmicId',left_on='COSMIC_ID',how='left')\n",
        "vector_rep_test.dropna(subset = [\"exp_ABCB1\"], inplace=True)\n",
        "vector_rep_test"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COSMIC_ID</th>\n",
              "      <th>AUC</th>\n",
              "      <th>exp_ABCB1</th>\n",
              "      <th>exp_ACE2</th>\n",
              "      <th>exp_ACHE</th>\n",
              "      <th>exp_ADORA1</th>\n",
              "      <th>exp_ADORA2B</th>\n",
              "      <th>exp_ADRA1A</th>\n",
              "      <th>exp_ADRA2B</th>\n",
              "      <th>exp_AKT1</th>\n",
              "      <th>exp_ANO1</th>\n",
              "      <th>exp_ANPEP</th>\n",
              "      <th>exp_APH1A</th>\n",
              "      <th>exp_APH1B</th>\n",
              "      <th>exp_ARID1A</th>\n",
              "      <th>exp_ARID1B</th>\n",
              "      <th>exp_ARID2</th>\n",
              "      <th>exp_ARID4A</th>\n",
              "      <th>exp_ARID4B</th>\n",
              "      <th>exp_ASCC3</th>\n",
              "      <th>exp_ASH1L</th>\n",
              "      <th>exp_ASXL1</th>\n",
              "      <th>exp_ATF7IP</th>\n",
              "      <th>exp_ATM</th>\n",
              "      <th>exp_ATP1A1</th>\n",
              "      <th>exp_ATP1A2</th>\n",
              "      <th>exp_ATR</th>\n",
              "      <th>exp_ATRX</th>\n",
              "      <th>exp_AUTS2</th>\n",
              "      <th>exp_AVPR2</th>\n",
              "      <th>exp_BACE1</th>\n",
              "      <th>exp_BCL2L1</th>\n",
              "      <th>exp_BCL2L10</th>\n",
              "      <th>exp_BCOR</th>\n",
              "      <th>exp_BIRC2</th>\n",
              "      <th>exp_BPTF</th>\n",
              "      <th>exp_BRAF</th>\n",
              "      <th>exp_BRCA1</th>\n",
              "      <th>exp_BRIP1</th>\n",
              "      <th>exp_CA1</th>\n",
              "      <th>...</th>\n",
              "      <th>loss_SLC29A1</th>\n",
              "      <th>loss_SLC6A4</th>\n",
              "      <th>loss_SLX4</th>\n",
              "      <th>loss_SMAD4</th>\n",
              "      <th>loss_SMARCA4</th>\n",
              "      <th>loss_SMC3</th>\n",
              "      <th>loss_SMC4</th>\n",
              "      <th>loss_SOAT1</th>\n",
              "      <th>loss_SOST</th>\n",
              "      <th>loss_SRCAP</th>\n",
              "      <th>loss_STEAP1</th>\n",
              "      <th>loss_TACR1</th>\n",
              "      <th>loss_TAF1</th>\n",
              "      <th>loss_TBXAS1</th>\n",
              "      <th>loss_TEP1</th>\n",
              "      <th>loss_TET2</th>\n",
              "      <th>loss_TFRC</th>\n",
              "      <th>loss_THRA</th>\n",
              "      <th>loss_TLR4</th>\n",
              "      <th>loss_TNC</th>\n",
              "      <th>loss_TNFRSF8</th>\n",
              "      <th>loss_TNFSF4</th>\n",
              "      <th>loss_TNNC1</th>\n",
              "      <th>loss_TP53</th>\n",
              "      <th>loss_TP53BP1</th>\n",
              "      <th>loss_TPR</th>\n",
              "      <th>loss_TRIM37</th>\n",
              "      <th>loss_TTF1</th>\n",
              "      <th>loss_TUBB4B</th>\n",
              "      <th>loss_UBA3</th>\n",
              "      <th>loss_UBR5</th>\n",
              "      <th>loss_UGCG</th>\n",
              "      <th>loss_UMPS</th>\n",
              "      <th>loss_USP36</th>\n",
              "      <th>loss_VWF</th>\n",
              "      <th>loss_WDR70</th>\n",
              "      <th>loss_XPNPEP1</th>\n",
              "      <th>loss_YEATS2</th>\n",
              "      <th>loss_ZBTB20</th>\n",
              "      <th>loss_ZNF451</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>687514</td>\n",
              "      <td>0.940797</td>\n",
              "      <td>-0.795813</td>\n",
              "      <td>6.750864</td>\n",
              "      <td>-1.348356</td>\n",
              "      <td>-0.507384</td>\n",
              "      <td>0.858196</td>\n",
              "      <td>-1.139359</td>\n",
              "      <td>-0.539454</td>\n",
              "      <td>-0.670917</td>\n",
              "      <td>0.312023</td>\n",
              "      <td>-0.571699</td>\n",
              "      <td>-0.279029</td>\n",
              "      <td>-1.313566</td>\n",
              "      <td>-1.004221</td>\n",
              "      <td>-0.201694</td>\n",
              "      <td>0.140703</td>\n",
              "      <td>1.153767</td>\n",
              "      <td>-0.936723</td>\n",
              "      <td>0.031886</td>\n",
              "      <td>-0.922144</td>\n",
              "      <td>-1.335071</td>\n",
              "      <td>0.383756</td>\n",
              "      <td>-0.426571</td>\n",
              "      <td>0.561121</td>\n",
              "      <td>-1.351629</td>\n",
              "      <td>0.454086</td>\n",
              "      <td>0.740559</td>\n",
              "      <td>-0.153443</td>\n",
              "      <td>-0.766455</td>\n",
              "      <td>1.576346</td>\n",
              "      <td>-0.223903</td>\n",
              "      <td>-0.496992</td>\n",
              "      <td>1.397938</td>\n",
              "      <td>0.093870</td>\n",
              "      <td>0.034423</td>\n",
              "      <td>0.713784</td>\n",
              "      <td>0.708052</td>\n",
              "      <td>1.223724</td>\n",
              "      <td>-0.308725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>687563</td>\n",
              "      <td>0.929480</td>\n",
              "      <td>-0.436656</td>\n",
              "      <td>-0.408545</td>\n",
              "      <td>0.233556</td>\n",
              "      <td>2.160566</td>\n",
              "      <td>0.351251</td>\n",
              "      <td>-0.952091</td>\n",
              "      <td>0.522925</td>\n",
              "      <td>-0.370882</td>\n",
              "      <td>-0.463191</td>\n",
              "      <td>-0.211496</td>\n",
              "      <td>1.112712</td>\n",
              "      <td>0.798567</td>\n",
              "      <td>-1.190600</td>\n",
              "      <td>-1.307199</td>\n",
              "      <td>-0.521974</td>\n",
              "      <td>-0.666628</td>\n",
              "      <td>0.438484</td>\n",
              "      <td>-1.113651</td>\n",
              "      <td>0.602878</td>\n",
              "      <td>-0.809396</td>\n",
              "      <td>-1.006598</td>\n",
              "      <td>-0.853497</td>\n",
              "      <td>-1.207747</td>\n",
              "      <td>1.418919</td>\n",
              "      <td>0.580894</td>\n",
              "      <td>0.526905</td>\n",
              "      <td>-0.701879</td>\n",
              "      <td>-0.440817</td>\n",
              "      <td>0.293079</td>\n",
              "      <td>0.733459</td>\n",
              "      <td>-0.477606</td>\n",
              "      <td>-1.427385</td>\n",
              "      <td>-0.446230</td>\n",
              "      <td>0.101863</td>\n",
              "      <td>-1.157947</td>\n",
              "      <td>-0.430523</td>\n",
              "      <td>-0.866720</td>\n",
              "      <td>0.008396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>687804</td>\n",
              "      <td>0.986062</td>\n",
              "      <td>-0.576246</td>\n",
              "      <td>-0.395293</td>\n",
              "      <td>0.685969</td>\n",
              "      <td>1.306140</td>\n",
              "      <td>-1.212135</td>\n",
              "      <td>0.272033</td>\n",
              "      <td>-0.057714</td>\n",
              "      <td>0.141770</td>\n",
              "      <td>-0.671072</td>\n",
              "      <td>-0.530651</td>\n",
              "      <td>-1.094255</td>\n",
              "      <td>-0.671018</td>\n",
              "      <td>1.710095</td>\n",
              "      <td>1.180384</td>\n",
              "      <td>1.069085</td>\n",
              "      <td>-0.371779</td>\n",
              "      <td>1.200086</td>\n",
              "      <td>0.242445</td>\n",
              "      <td>1.515254</td>\n",
              "      <td>0.093265</td>\n",
              "      <td>0.079861</td>\n",
              "      <td>0.490471</td>\n",
              "      <td>1.067641</td>\n",
              "      <td>-0.850431</td>\n",
              "      <td>-0.612285</td>\n",
              "      <td>1.874295</td>\n",
              "      <td>1.178846</td>\n",
              "      <td>-0.179135</td>\n",
              "      <td>2.159313</td>\n",
              "      <td>-0.364359</td>\n",
              "      <td>-0.621672</td>\n",
              "      <td>1.875189</td>\n",
              "      <td>-1.385692</td>\n",
              "      <td>0.780545</td>\n",
              "      <td>-0.032411</td>\n",
              "      <td>0.983732</td>\n",
              "      <td>0.962647</td>\n",
              "      <td>-0.053273</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>687812</td>\n",
              "      <td>0.970363</td>\n",
              "      <td>-0.407134</td>\n",
              "      <td>0.302041</td>\n",
              "      <td>-0.569647</td>\n",
              "      <td>-0.444363</td>\n",
              "      <td>-0.002900</td>\n",
              "      <td>-0.767819</td>\n",
              "      <td>0.800654</td>\n",
              "      <td>-1.433001</td>\n",
              "      <td>0.020576</td>\n",
              "      <td>-0.429900</td>\n",
              "      <td>0.963898</td>\n",
              "      <td>-0.196901</td>\n",
              "      <td>0.021789</td>\n",
              "      <td>-0.477380</td>\n",
              "      <td>0.291777</td>\n",
              "      <td>0.901228</td>\n",
              "      <td>1.544993</td>\n",
              "      <td>-1.259742</td>\n",
              "      <td>0.354537</td>\n",
              "      <td>1.266232</td>\n",
              "      <td>-0.692246</td>\n",
              "      <td>0.287597</td>\n",
              "      <td>-1.269117</td>\n",
              "      <td>0.094093</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>0.147089</td>\n",
              "      <td>-0.899760</td>\n",
              "      <td>-0.834839</td>\n",
              "      <td>0.271643</td>\n",
              "      <td>-0.497657</td>\n",
              "      <td>-0.470703</td>\n",
              "      <td>-0.973290</td>\n",
              "      <td>0.672146</td>\n",
              "      <td>0.876057</td>\n",
              "      <td>-0.131739</td>\n",
              "      <td>-1.399480</td>\n",
              "      <td>-0.352010</td>\n",
              "      <td>-0.235992</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>687829</td>\n",
              "      <td>0.945158</td>\n",
              "      <td>-0.769827</td>\n",
              "      <td>-0.436544</td>\n",
              "      <td>-1.744371</td>\n",
              "      <td>0.028044</td>\n",
              "      <td>-0.583775</td>\n",
              "      <td>-2.015895</td>\n",
              "      <td>0.587347</td>\n",
              "      <td>1.260093</td>\n",
              "      <td>-0.391733</td>\n",
              "      <td>-0.189984</td>\n",
              "      <td>-0.714687</td>\n",
              "      <td>2.609823</td>\n",
              "      <td>1.279121</td>\n",
              "      <td>0.083797</td>\n",
              "      <td>0.303480</td>\n",
              "      <td>0.220517</td>\n",
              "      <td>-0.170499</td>\n",
              "      <td>0.775564</td>\n",
              "      <td>-1.260783</td>\n",
              "      <td>2.199182</td>\n",
              "      <td>0.646782</td>\n",
              "      <td>0.607739</td>\n",
              "      <td>-1.076049</td>\n",
              "      <td>-1.485002</td>\n",
              "      <td>-0.563865</td>\n",
              "      <td>0.759282</td>\n",
              "      <td>-0.103964</td>\n",
              "      <td>-0.244284</td>\n",
              "      <td>0.182616</td>\n",
              "      <td>-0.374443</td>\n",
              "      <td>-0.035760</td>\n",
              "      <td>1.242763</td>\n",
              "      <td>1.398347</td>\n",
              "      <td>0.223538</td>\n",
              "      <td>1.286942</td>\n",
              "      <td>1.084581</td>\n",
              "      <td>-0.274262</td>\n",
              "      <td>-0.489478</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>1331033</td>\n",
              "      <td>0.995324</td>\n",
              "      <td>-0.352761</td>\n",
              "      <td>-0.171732</td>\n",
              "      <td>1.592927</td>\n",
              "      <td>-0.286210</td>\n",
              "      <td>-1.546369</td>\n",
              "      <td>1.061814</td>\n",
              "      <td>-0.501232</td>\n",
              "      <td>-0.596108</td>\n",
              "      <td>-0.480727</td>\n",
              "      <td>-0.473688</td>\n",
              "      <td>1.256813</td>\n",
              "      <td>-0.338173</td>\n",
              "      <td>0.755688</td>\n",
              "      <td>0.402144</td>\n",
              "      <td>0.101092</td>\n",
              "      <td>1.209557</td>\n",
              "      <td>0.351939</td>\n",
              "      <td>0.985177</td>\n",
              "      <td>-0.351734</td>\n",
              "      <td>0.132014</td>\n",
              "      <td>-1.120571</td>\n",
              "      <td>1.265195</td>\n",
              "      <td>-0.843612</td>\n",
              "      <td>-0.247218</td>\n",
              "      <td>2.029284</td>\n",
              "      <td>-0.119203</td>\n",
              "      <td>0.418137</td>\n",
              "      <td>1.090227</td>\n",
              "      <td>-1.112849</td>\n",
              "      <td>-1.406083</td>\n",
              "      <td>-0.119738</td>\n",
              "      <td>-0.149195</td>\n",
              "      <td>-0.385561</td>\n",
              "      <td>0.861071</td>\n",
              "      <td>0.170410</td>\n",
              "      <td>2.742696</td>\n",
              "      <td>2.731487</td>\n",
              "      <td>0.144998</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>1331048</td>\n",
              "      <td>0.955483</td>\n",
              "      <td>2.043625</td>\n",
              "      <td>-0.108627</td>\n",
              "      <td>0.188126</td>\n",
              "      <td>-0.149335</td>\n",
              "      <td>-1.465629</td>\n",
              "      <td>-0.098200</td>\n",
              "      <td>0.329278</td>\n",
              "      <td>-1.257088</td>\n",
              "      <td>-0.459454</td>\n",
              "      <td>-0.493387</td>\n",
              "      <td>-1.064967</td>\n",
              "      <td>2.109026</td>\n",
              "      <td>0.418619</td>\n",
              "      <td>0.961348</td>\n",
              "      <td>1.174797</td>\n",
              "      <td>1.635404</td>\n",
              "      <td>2.196770</td>\n",
              "      <td>1.231665</td>\n",
              "      <td>1.594054</td>\n",
              "      <td>0.589881</td>\n",
              "      <td>-1.427284</td>\n",
              "      <td>2.758338</td>\n",
              "      <td>-1.254299</td>\n",
              "      <td>0.845213</td>\n",
              "      <td>0.205830</td>\n",
              "      <td>0.263174</td>\n",
              "      <td>0.214168</td>\n",
              "      <td>0.221818</td>\n",
              "      <td>-0.796457</td>\n",
              "      <td>-0.477848</td>\n",
              "      <td>0.167692</td>\n",
              "      <td>1.345630</td>\n",
              "      <td>-0.269242</td>\n",
              "      <td>1.287716</td>\n",
              "      <td>-0.178229</td>\n",
              "      <td>-1.487758</td>\n",
              "      <td>-0.896879</td>\n",
              "      <td>0.036836</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>1480359</td>\n",
              "      <td>0.974871</td>\n",
              "      <td>-0.637749</td>\n",
              "      <td>-0.038341</td>\n",
              "      <td>-0.738998</td>\n",
              "      <td>2.386970</td>\n",
              "      <td>0.062885</td>\n",
              "      <td>-0.252525</td>\n",
              "      <td>0.122702</td>\n",
              "      <td>-1.044753</td>\n",
              "      <td>-0.135886</td>\n",
              "      <td>-0.458211</td>\n",
              "      <td>0.769244</td>\n",
              "      <td>-1.096517</td>\n",
              "      <td>-1.079964</td>\n",
              "      <td>-1.531456</td>\n",
              "      <td>0.309620</td>\n",
              "      <td>0.629439</td>\n",
              "      <td>2.017442</td>\n",
              "      <td>-0.130601</td>\n",
              "      <td>1.264682</td>\n",
              "      <td>0.074173</td>\n",
              "      <td>0.792520</td>\n",
              "      <td>-0.542115</td>\n",
              "      <td>0.156878</td>\n",
              "      <td>-0.394853</td>\n",
              "      <td>-0.223736</td>\n",
              "      <td>1.952097</td>\n",
              "      <td>0.670500</td>\n",
              "      <td>-0.360287</td>\n",
              "      <td>-1.005924</td>\n",
              "      <td>-0.257144</td>\n",
              "      <td>-0.364049</td>\n",
              "      <td>-0.754693</td>\n",
              "      <td>0.806750</td>\n",
              "      <td>-1.001954</td>\n",
              "      <td>1.233887</td>\n",
              "      <td>-0.798996</td>\n",
              "      <td>-0.282263</td>\n",
              "      <td>-0.219475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>1480374</td>\n",
              "      <td>0.930763</td>\n",
              "      <td>-0.465433</td>\n",
              "      <td>-0.535247</td>\n",
              "      <td>-0.004707</td>\n",
              "      <td>0.211310</td>\n",
              "      <td>-0.722655</td>\n",
              "      <td>-0.403263</td>\n",
              "      <td>-0.633580</td>\n",
              "      <td>-0.618564</td>\n",
              "      <td>-0.064310</td>\n",
              "      <td>-0.689388</td>\n",
              "      <td>0.195847</td>\n",
              "      <td>-1.245898</td>\n",
              "      <td>-0.036047</td>\n",
              "      <td>-0.355331</td>\n",
              "      <td>0.104380</td>\n",
              "      <td>-0.556177</td>\n",
              "      <td>0.592252</td>\n",
              "      <td>-0.867687</td>\n",
              "      <td>0.447899</td>\n",
              "      <td>0.178316</td>\n",
              "      <td>2.522514</td>\n",
              "      <td>-0.483700</td>\n",
              "      <td>0.145005</td>\n",
              "      <td>-0.682858</td>\n",
              "      <td>0.308657</td>\n",
              "      <td>-0.199848</td>\n",
              "      <td>-0.890640</td>\n",
              "      <td>-1.453714</td>\n",
              "      <td>-0.335680</td>\n",
              "      <td>1.342201</td>\n",
              "      <td>-0.424633</td>\n",
              "      <td>-0.834105</td>\n",
              "      <td>-0.850663</td>\n",
              "      <td>-0.254813</td>\n",
              "      <td>1.676032</td>\n",
              "      <td>-1.347209</td>\n",
              "      <td>-0.566037</td>\n",
              "      <td>-0.044752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>1524415</td>\n",
              "      <td>0.731406</td>\n",
              "      <td>0.568588</td>\n",
              "      <td>-0.264937</td>\n",
              "      <td>-0.130998</td>\n",
              "      <td>3.875744</td>\n",
              "      <td>-0.482345</td>\n",
              "      <td>0.095507</td>\n",
              "      <td>-0.559068</td>\n",
              "      <td>-1.088956</td>\n",
              "      <td>-0.604051</td>\n",
              "      <td>0.493358</td>\n",
              "      <td>-0.573980</td>\n",
              "      <td>-1.117603</td>\n",
              "      <td>-0.975416</td>\n",
              "      <td>0.102054</td>\n",
              "      <td>0.346760</td>\n",
              "      <td>-1.366181</td>\n",
              "      <td>-0.349160</td>\n",
              "      <td>0.057235</td>\n",
              "      <td>-0.269184</td>\n",
              "      <td>-0.186325</td>\n",
              "      <td>-0.180664</td>\n",
              "      <td>0.564814</td>\n",
              "      <td>-0.220911</td>\n",
              "      <td>-0.614742</td>\n",
              "      <td>0.756898</td>\n",
              "      <td>0.877868</td>\n",
              "      <td>-0.689097</td>\n",
              "      <td>-0.251284</td>\n",
              "      <td>-0.887376</td>\n",
              "      <td>0.947362</td>\n",
              "      <td>-0.305558</td>\n",
              "      <td>-0.672281</td>\n",
              "      <td>0.867460</td>\n",
              "      <td>0.012339</td>\n",
              "      <td>-0.975371</td>\n",
              "      <td>0.418031</td>\n",
              "      <td>-0.035948</td>\n",
              "      <td>0.038981</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1462 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     COSMIC_ID       AUC  exp_ABCB1  ...  loss_YEATS2  loss_ZBTB20  loss_ZNF451\n",
              "13      687514  0.940797  -0.795813  ...          0.0          0.0          0.0\n",
              "15      687563  0.929480  -0.436656  ...          0.0          0.0          0.0\n",
              "30      687804  0.986062  -0.576246  ...          0.0          0.0          0.0\n",
              "32      687812  0.970363  -0.407134  ...          0.0          0.0          0.0\n",
              "37      687829  0.945158  -0.769827  ...          0.0          0.0          0.0\n",
              "..         ...       ...        ...  ...          ...          ...          ...\n",
              "819    1331033  0.995324  -0.352761  ...          0.0          0.0          0.0\n",
              "828    1331048  0.955483   2.043625  ...          0.0          0.0          0.0\n",
              "835    1480359  0.974871  -0.637749  ...          0.0          0.0          0.0\n",
              "843    1480374  0.930763  -0.465433  ...          0.0          0.0          0.0\n",
              "858    1524415  0.731406   0.568588  ...          0.0          0.0          0.0\n",
              "\n",
              "[100 rows x 1462 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XerNZCyJ4vN_"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, ParameterGrid, RandomizedSearchCV, ParameterSampler\n",
        "from numpy import linspace \n",
        "from numpy import sqrt\n",
        "from numpy import corrcoef\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from scipy.stats import norm , truncnorm"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpghSUzL3OsV"
      },
      "source": [
        "#xgboost\n",
        "\n",
        "\n",
        "\n",
        "lower, upper = 0, 10\n",
        "mu, sigma = 0.3, 0.2\n",
        "\n",
        "flag = True\n",
        "\n",
        "\n",
        "\n",
        "X_train = vector_rep_train.drop(['AUC','COSMIC_ID'],axis=1)\n",
        "y_train = vector_rep_train['AUC']\n",
        "X_test = vector_rep_test.drop(['AUC','COSMIC_ID'],axis=1)\n",
        "y_test = vector_rep_test['AUC']\n",
        "\n",
        "        # Instantiate the model\n",
        "xgboost = xgb.XGBRegressor(seed = 2, objective = 'reg:squarederror', n_jobs = 4, booster = 'gbtree')\n",
        "# Hyperparamter tuning on trainig set\n",
        "param_test1 = {'max_depth':[2,3,4,5,6,7,8],\n",
        "            'subsample':[0.5,1],\n",
        "            'learning_rate':truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma),\n",
        "            'n_estimators' : [25,50 ,75, 101,100,102],\n",
        "            'reg_lambda' :[0., 0.5,1., 5]\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        # Instatiate grid search object\n",
        "grid = RandomizedSearchCV(estimator = xgboost, param_distributions = param_test1,\n",
        "                    scoring=\"neg_mean_squared_error\",\n",
        "                    cv=5,\n",
        "                    n_jobs = 4,\n",
        "                    refit=True,\n",
        "                    n_iter = 50,\n",
        "                    random_state = 2)\n",
        "\n",
        "#Train the grid (compute cross validation for 50 parameter combinations)\n",
        "try:\n",
        "    grid.fit(X_train, y_train)\n",
        "except ValueError:\n",
        "    i+= 1\n",
        "    flag = False\n",
        "    glob = False\n",
        "    print('upss  lek: ' + str(drug)+' random state: ' + str(random_state)+ ' seed: '+str(seeds[i])\n",
        "          + 'proba nr: ' +str(num))\n",
        "    rejects.append(drug)\n",
        "\n",
        "if flag:\n",
        "\n",
        "    best_model_test_predictions = grid.predict(X_test)\n",
        "    best_model_train_predictions = grid.predict(X_train)\n",
        "\n",
        "    #y_test\n",
        "    RMSE_test = sqrt(mean_squared_error(y_test, best_model_test_predictions))\n",
        "    corr_test = corrcoef(y_test, best_model_test_predictions)[0][1]\n",
        "    # if isnan(corr_test)==True:\n",
        "    #     corr_test = 0\n",
        "\n",
        "    #y_train\n",
        "    RMSE_train = sqrt(mean_squared_error(y_train, best_model_train_predictions))\n",
        "    corr_train = corrcoef(y_train , best_model_train_predictions)[0][1]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwTcEBx74d3a",
        "outputId": "2d38446e-b540-472c-af55-9c1c06a16a8c"
      },
      "source": [
        "corr_train, RMSE_train,corr_test, RMSE_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9907310733263157,\n",
              " 0.028790097944276245,\n",
              " 0.5459464313109172,\n",
              " 0.1745090858526683)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71O5IigvjcJx",
        "outputId": "45ec5d16-931c-4720-e1d3-b056a760e25d"
      },
      "source": [
        "#elatsic_net for each drug \n",
        "\n",
        "X_train = vector_rep_train.drop(['AUC','COSMIC_ID'],axis=1)\n",
        "y_train = vector_rep_train['AUC']\n",
        "X_test = vector_rep_test.drop(['AUC','COSMIC_ID'],axis=1)\n",
        "y_test = vector_rep_test['AUC']\n",
        "\n",
        "# Hyperparamter tuning on trainig set\n",
        "# Instantiate the model\n",
        "elastic_net = ElasticNet(random_state= 2, tol = 1)\n",
        "# Instanitiate scaler\n",
        "scaler = StandardScaler()\n",
        "# Initiate pipeline\n",
        "pipeline = Pipeline([\n",
        "        (\"scaler\", scaler),\n",
        "        (\"estimator\", elastic_net)\n",
        "        ])\n",
        "param_grid = {\"estimator__alpha\": [0.001, 0.01, 0.1, 1.],\n",
        "            \"estimator__l1_ratio\": [0., 0.25, 0.5, 0.75, 1.]}\n",
        "# Instatiate grid search object\n",
        "grid = GridSearchCV(pipeline, param_grid,\n",
        "                scoring=\"neg_mean_squared_error\",\n",
        "                cv=5,\n",
        "                refit=True)\n",
        "grid.fit(X_train, y_train)\n",
        "#second param_grid ( only if alpha !=1)\n",
        "alpha = grid.best_params_['estimator__alpha']\n",
        "if alpha != 1:\n",
        "    param_grid = {\"estimator__alpha\": list(linspace(alpha, 10*alpha,4)),\n",
        "            \"estimator__l1_ratio\": [0., 0.25, 0.5, 0.75, 1.]}\n",
        "    grid = GridSearchCV(pipeline, param_grid,\n",
        "                scoring=\"neg_mean_squared_error\",\n",
        "                cv=5,\n",
        "                refit=True)\n",
        "\n",
        "\n",
        "# Train the grid (compute cross validation for every parameter combination)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "\n",
        "\n",
        "best_model_test_predictions = grid.predict(X_test)\n",
        "best_model_train_predictions = grid.predict(X_train)\n",
        "\n",
        "#y_test\n",
        "RMSE_test = sqrt(mean_squared_error(y_test, best_model_test_predictions))\n",
        "\n",
        "corr_test = corrcoef(y_test, best_model_test_predictions)[0][1]\n",
        "\n",
        "\n",
        "#y_train\n",
        "RMSE_train = sqrt(mean_squared_error(y_train, best_model_train_predictions))\n",
        "\n",
        "corr_train = corrcoef(y_train , best_model_train_predictions)[0][1]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'estimator__alpha': 0.04, 'estimator__l1_ratio': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eXipFJw6RCk",
        "outputId": "5c7ff622-5f2f-4a99-8e92-b3da2e646977"
      },
      "source": [
        "corr_train, RMSE_train,corr_test, RMSE_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7710332129874308,\n",
              " 0.12111072494374298,\n",
              " 0.5653321234445876,\n",
              " 0.17079064909733452)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijHHedMmWf-4",
        "outputId": "db34c57f-3229-4580-b868-f76fd4843a75"
      },
      "source": [
        "min(grid.best_estimator_['estimator'].coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.035662322964778566"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2x_ESYoisoX",
        "outputId": "59de95bc-7f66-458c-eb22-c44c9589b6e1"
      },
      "source": [
        "grid.best_estimator_['estimator'].coef_[99]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.035662322964778566"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l5VNjIUhPzm",
        "outputId": "6b2526f5-434e-4092-d88e-f84db9393c90"
      },
      "source": [
        "list(grid.best_estimator_['estimator'].coef_).index(max(grid.best_estimator_['estimator'].coef_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sxwIJf0mh_5M",
        "outputId": "d32553cf-153c-499f-81bd-05dbd6604903"
      },
      "source": [
        "X_train.columns[99]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'exp_EDNRB'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru_dX9X5Vd7s"
      },
      "source": [
        "Train_graphs = CosmicGraph('',Train_set,'Train_set_graph.dataset')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kG6CNYbWszS"
      },
      "source": [
        "Test_graphs = CosmicGraph('',Test_set,'Test_set_graph1.dataset')\n",
        "Validation_graphs = CosmicGraph('',Validation_set,'Validation_set_graph1.dataset')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu4eDBH4DQHU",
        "outputId": "cfa34b5b-eade-4111-e321-4b9db9b03168"
      },
      "source": [
        "d = Train_graphs[0]\n",
        "print(d.y)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9168]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oHkq3kZWqY6",
        "outputId": "2026b7f6-a899-475d-a005-c11895208902"
      },
      "source": [
        "Test_graphs.num_classes"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru-SKbFTFOX2"
      },
      "source": [
        "data = Train_graphs\n",
        "number_of_classes=655"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cFWKftRtF0-"
      },
      "source": [
        "m = torch.nn.Dropout(p=0.2)\n",
        "input = torch.randn(20, 16)\n",
        "output = m(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbiVluDGpf6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b55726e-e65a-4d01-cd93-2dace57bf8c4"
      },
      "source": [
        "!pip install ray[rllib]==0.8.1 "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[rllib]==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (21.0)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (0.4.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.6.4)\n",
            "Requirement already satisfied: redis>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.5.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (0.8.9)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (4.5.4.58)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.4.1)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (0.17.3)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.1.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]->ray[rllib]==0.8.1) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]->ray[rllib]==0.8.1) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]->ray[rllib]==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]->ray[rllib]==0.8.1) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]->ray[rllib]==0.8.1) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ray[rllib]==0.8.1) (2.4.7)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (8.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Iwzpx_njvkt"
      },
      "source": [
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import track\n",
        "from functools import partial\n",
        "import os"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eWmGMvfp_59"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVw6LOd8EkEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc13514f-7f9d-46ae-ce87-e56523346f09"
      },
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F \n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp, global_add_pool as gdp\n",
        "embedding_size = 64\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.conv1 = GCNConv(data.num_features, embedding_size)\n",
        "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
        "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
        "        # Output layer\n",
        "        self.out = Linear(embedding_size, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.conv1(x, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "        #hidden = F.relu(hidden)\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "          \n",
        "        # Global Pooling (stack different aggregations)\n",
        "        # hidden = torch.cat([gap(hidden, batch_index), \n",
        "        #                     gmp(hidden, batch_index)], dim=1)\n",
        "        hidden = gmp(hidden, batch_index)\n",
        "        # print(hidden.shape)\n",
        "        # Apply a final (linear) classifier.\n",
        "        \n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "model = GCN()\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(5, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Number of parameters:  8769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kB_AGKqCx0"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi2fBqmdp-Vd"
      },
      "source": [
        "def train():\n",
        "    # Enumerate over the data\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "      # Use GPU\n",
        "      i+=1\n",
        "      batch.to(device)\n",
        "        \n",
        "      # Reset gradients\n",
        "      optimizer.zero_grad() \n",
        "      # Passing the node features and the connection info\n",
        "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
        "      \n",
        "      # Calculating the loss and gradients\n",
        "      loss = loss_fn(pred, batch.y)  \n",
        "      epoch_loss += loss.item()    \n",
        "      loss.backward()  \n",
        "      # Update using the gradients\n",
        "      optimizer.step()\n",
        "    epoch_loss = epoch_loss/i   \n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def train_val():\n",
        "    # Enumerate over the data\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    for batch in val_loader:\n",
        "      # Use GPU\n",
        "      i+=1\n",
        "      batch.to(device)\n",
        "        \n",
        "      # Reset gradients\n",
        "      optimizer.zero_grad() \n",
        "      # Passing the node features and the connection info\n",
        "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
        "      \n",
        "      # Calculating the loss and gradients\n",
        "      loss = loss_fn(pred, batch.y)  \n",
        "      epoch_loss += loss.item()    \n",
        "      loss.backward()  \n",
        "      # Update using the gradients\n",
        "      optimizer.step()\n",
        "    epoch_loss = epoch_loss/i   \n",
        "    return epoch_loss\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia4gvTgMFYum"
      },
      "source": [
        "\n",
        "# Root mean squared error\n",
        "model = GCN()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.25)  \n",
        "\n",
        "# Use GPU for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Wrap data in a data loader\n",
        "data_size = len(data)\n",
        "NUM_GRAPHS_PER_BATCH = 16\n",
        "train_loader = DataLoader(Train_graphs, \n",
        "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "val_loader = DataLoader(Validation_graphs, \n",
        "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "test_loader = DataLoader(Test_graphs, \n",
        "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "losses = []\n",
        "for epoch in range(500):\n",
        "    loss = train()\n",
        "    losses.append(loss)\n",
        "    if epoch % 100 == 0:\n",
        "      print(f\"Epoch {epoch} | Train Loss {loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp63KZWGt-Px"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d640XkEI9T7z"
      },
      "source": [
        "# losses_float = [float(loss.cpu().detach().numpy()) for loss in losses] \n",
        "import seaborn as sns\n",
        "loss_indices = [i for i,l in enumerate(losses)] \n",
        "plt = sns.lineplot(loss_indices, losses)\n",
        "plt"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeYV2wU1Vj1o"
      },
      "source": [
        "def train_graphs(config, checkpoint_dir=None, data_dir=None):\n",
        "  \n",
        "  results = {}\n",
        "  results['config'] = config\n",
        "  model = GCN()\n",
        "  \n",
        "  loss_fn = torch.nn.MSELoss()\n",
        "  print(config[\"lr\"])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2\"])  \n",
        "  track.init()\n",
        "  # # Use GPU for training\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "\n",
        "  # # Wrap data in a data loader\n",
        "  data_size = len(data)\n",
        "  print(data_size)\n",
        "  # # NUM_GRAPHS_PER_BATCH = config[\"batch_size\"]\n",
        "  NUM_GRAPHS_PER_BATCH = 32\n",
        "  train_loader = DataLoader(Train_graphs, \n",
        "                      batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "  val_loader = DataLoader(Validation_graphs, \n",
        "                      batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "  test_loader = DataLoader(Test_graphs, \n",
        "                          batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "  losses=[]\n",
        "  val_losses=[]\n",
        "  for epoch in range(10):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "      # Use GPU\n",
        "      i+=1\n",
        "      batch.to(device)\n",
        "        \n",
        "      # Reset gradients\n",
        "      optimizer.zero_grad() \n",
        "      # Passing the node features and the connection info\n",
        "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
        "      \n",
        "      # Calculating the loss and gradients\n",
        "      loss = loss_fn(pred, batch.y)  \n",
        "      epoch_loss += loss.item()    \n",
        "      loss.backward()  \n",
        "      # Update using the gradients\n",
        "      optimizer.step()\n",
        "    epoch_loss = epoch_loss/i   \n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    i = 0\n",
        "    for batch in val_loader:\n",
        "      # Use GPU\n",
        "      i+=1\n",
        "      batch.to(device)\n",
        "        \n",
        "      # Reset gradients\n",
        "      optimizer.zero_grad() \n",
        "      # Passing the node features and the connection info\n",
        "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
        "      \n",
        "      # Calculating the loss and gradients\n",
        "      loss = loss_fn(pred, batch.y)  \n",
        "      epoch_loss += loss.item()    \n",
        "      loss.backward()  \n",
        "      # Update using the gradients\n",
        "      optimizer.step()\n",
        "    epoch_loss = epoch_loss/i\n",
        "    val_losses.append(epoch_loss)\n",
        "    # loss_val = train_val()\n",
        "    \n",
        "    # with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "    #   path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "    #   torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "    track.log(loss=epoch_loss, accuracy=epoch_loss)\n",
        "  print(config)\n",
        "  loss_indices = [i for i,l in enumerate(losses)] \n",
        "  sns.lineplot(loss_indices, losses)\n",
        "  plt.show()\n",
        "  loss_indices = [i for i,l in enumerate(val_losses)] \n",
        "  sns.lineplot(loss_indices, val_losses)\n",
        "  plt.show()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJmo3F7Tt9EW"
      },
      "source": [
        "config = {\n",
        "    \"l2\": tune.loguniform(1e-4, 1e-1),\n",
        "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "    # \"batch_size\": tune.choice([8,16,32,64])\n",
        "}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lGSWgOYr0Rs",
        "outputId": "e22dbd0a-2b78-4a68-be5f-4a9323d7ae2a"
      },
      "source": [
        "tune.sample_from(lambda _: 2**np.random.randint(2, 9))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ray.tune.sample.Function at 0x7f4bb45d00d0>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoOtxCDSq5F4",
        "outputId": "791b2324-c421-425b-8de8-efe1996914e7"
      },
      "source": [
        "\n",
        "analysis = tune.run(train_graphs, \n",
        "                        config=config, \n",
        "                        num_samples=5,\n",
        "                        # local_dir=global_experiment_dir,\n",
        "                        # resources_per_trial={\"cpu\": 1, \"gpu\": 0.48},\n",
        "                        max_failures=3,\n",
        "                        verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-04 07:48:46,913\tWARNING worker.py:1063 -- Warning: The actor WrappedFunc has size 160351572 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
            "2021-11-04 07:48:47,091\tWARNING util.py:136 -- The `start_trial` operation took 3.5334043502807617 seconds to complete, which may be a performance bottleneck.\n",
            "2021-11-04 07:48:50,269\tWARNING worker.py:1063 -- Warning: The actor WrappedFunc has size 160351572 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
            "2021-11-04 07:48:50,445\tWARNING util.py:136 -- The `start_trial` operation took 3.346251964569092 seconds to complete, which may be a performance bottleneck.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZv3PhUwrzkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068652e7-be67-42a5-8514-9fa2ddb02f15"
      },
      "source": [
        "res = analysis.dataframe\n",
        "res\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Analysis.dataframe of <ray.tune.analysis.experiment_analysis.ExperimentAnalysis object at 0x7f759fedd9d0>>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufwAgSQdqhZC"
      },
      "source": [
        "scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=500,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l5Hm6AubqN35",
        "outputId": "b6f578ac-f781-440a-eeb3-d1839d99c172"
      },
      "source": [
        "result = tune.run(\n",
        "    partial(train_cifar, data_dir=None),\n",
        "    # resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n",
        "    config=config,\n",
        "    # num_samples=num_samples,\n",
        "    scheduler=scheduler,\n",
        "    progress_reporter=reporter,\n",
        "    fail_fast=\"raise\",\n",
        "    checkpoint_at_end=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-28 11:42:11,745\tWARNING experiment.py:303 -- No name detected on trainable. Using DEFAULT.\n",
            "2021-10-28 11:42:11,748\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n",
            "2021-10-28 11:42:12,874\tWARNING callback.py:117 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "2021-10-28 11:42:15,933\tERROR ray_trial_executor.py:600 -- Trial DEFAULT_178ba_00000: Unexpected error starting runner.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 590, in start_trial\n",
            "    return self._start_trial(trial, checkpoint, train=train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 465, in _start_trial\n",
            "    runner = self._setup_remote_runner(trial)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/tune/ray_trial_executor.py\", line 382, in _setup_remote_runner\n",
            "    return full_actor_class.remote(**kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/actor.py\", line 501, in remote\n",
            "    override_environment_variables))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/util/tracing/tracing_helper.py\", line 371, in _invocation_actor_class_remote_span\n",
            "    return method(self, args, kwargs, *_args, **_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/actor.py\", line 715, in _remote\n",
            "    meta.method_meta.methods.keys())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/function_manager.py\", line 385, in export_actor_class\n",
            "    self._worker)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ray/_private/utils.py\", line 641, in check_oversized_function\n",
            "    raise ValueError(error)\n",
            "ValueError: The actor ImplicitFunc is too large (153 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
            "2021-10-28 11:42:17,940\tWARNING util.py:166 -- The `start_trial` operation took 3.899 s, which may be a performance bottleneck.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 1.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-28_11-42-12\n",
            "Number of trials: 1/1 (1 ERROR)\n",
            "+---------------------+----------+-------+--------------+-------------+-----------+\n",
            "| Trial name          | status   | loc   |   batch_size |          l2 |        lr |\n",
            "|---------------------+----------+-------+--------------+-------------+-----------|\n",
            "| DEFAULT_178ba_00000 | ERROR    |       |            2 | 0.000505616 | 0.0230726 |\n",
            "+---------------------+----------+-------+--------------+-------------+-----------+\n",
            "Number of errored trials: 1\n",
            "+---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name          |   # failures | error file                                                                                                                               |\n",
            "|---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| DEFAULT_178ba_00000 |            1 | /root/ray_results/DEFAULT_2021-10-28_11-42-12/DEFAULT_178ba_00000_0_batch_size=2,l2=0.00050562,lr=0.023073_2021-10-28_11-42-14/error.txt |\n",
            "+---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "== Status ==\n",
            "Memory usage on this node: 1.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 256.000: None | Iter 128.000: None | Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.39 GiB heap, 0.0/3.69 GiB objects\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-28_11-42-12\n",
            "Number of trials: 1/1 (1 ERROR)\n",
            "+---------------------+----------+-------+--------------+-------------+-----------+\n",
            "| Trial name          | status   | loc   |   batch_size |          l2 |        lr |\n",
            "|---------------------+----------+-------+--------------+-------------+-----------|\n",
            "| DEFAULT_178ba_00000 | ERROR    |       |            2 | 0.000505616 | 0.0230726 |\n",
            "+---------------------+----------+-------+--------------+-------------+-----------+\n",
            "Number of errored trials: 1\n",
            "+---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name          |   # failures | error file                                                                                                                               |\n",
            "|---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| DEFAULT_178ba_00000 |            1 | /root/ray_results/DEFAULT_2021-10-28_11-42-12/DEFAULT_178ba_00000_0_batch_size=2,l2=0.00050562,lr=0.023073_2021-10-28_11-42-14/error.txt |\n",
            "+---------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "ename": "TuneError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3bf21d3708a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfail_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     checkpoint_at_end=False)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [DEFAULT_178ba_00000])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "uUfmMH8LaRyz",
        "outputId": "e9dcfa55-a993-429d-cdf6-98218bece566"
      },
      "source": [
        "preds = []\n",
        "reals = []\n",
        "# Analyze the results for one batch\n",
        "# test_batch = next(iter(test_loader))\n",
        "# i=0\n",
        "with torch.no_grad():\n",
        "    for test_batch in train_loader:\n",
        "      test_batch.to(device)\n",
        "      pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch)\n",
        "      reals.extend(test_batch.y.tolist()) \n",
        "      preds.extend(pred.tolist())\n",
        "df = pd.DataFrame()\n",
        "df[\"y_real\"] = reals\n",
        "df[\"y_pred\"] = preds\n",
        "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
        "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
        "correlation = df.corr(method='pearson')\n",
        "correlation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_real</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_real</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_pred</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        y_real  y_pred\n",
              "y_real     1.0     NaN\n",
              "y_pred     NaN     NaN"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "fkoNLx_hJI4B",
        "outputId": "f4163981-dca4-4c77-b390-2f1bbf886e03"
      },
      "source": [
        "\n",
        "plt = sns.scatterplot(data=df, x=\"y_real\", y=\"y_pred\")\n",
        "plt.set(xlim=(0, 1))\n",
        "plt.set(ylim=(0, 1))\n",
        "plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4bd8c6f850>"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "EkyliKozKrw7",
        "outputId": "b02a3ff5-ce10-4779-923f-b73d86632b5d"
      },
      "source": [
        "correlation = df.corr(method='pearson')\n",
        "correlation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_real</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_real</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.80932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_pred</th>\n",
              "      <td>0.80932</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         y_real   y_pred\n",
              "y_real  1.00000  0.80932\n",
              "y_pred  0.80932  1.00000"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92cP-sDAEBie",
        "outputId": "520ce4b8-f373-4d67-ff1f-67bdf35d6521"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from math import sqrt\n",
        "print(sqrt(mse(df['y_real'],df['y_pred'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.018824875487310353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "BZYOtTD4VxeM",
        "outputId": "6e62a5f4-b029-4699-c34a-fd252c70fe1a"
      },
      "source": [
        "preds = []\n",
        "reals = []\n",
        "# Analyze the results for one batch\n",
        "# test_batch = next(iter(test_loader))\n",
        "# i=0\n",
        "with torch.no_grad():\n",
        "    for test_batch in test_loader:\n",
        "      test_batch.to(device)\n",
        "      pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch)\n",
        "      reals.extend(test_batch.y.tolist()) \n",
        "      preds.extend(pred.tolist())\n",
        "df = pd.DataFrame()\n",
        "df[\"y_real\"] = reals\n",
        "df[\"y_pred\"] = preds\n",
        "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
        "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
        "correlation = df.corr(method='pearson')\n",
        "correlation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_real</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_real</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.153272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y_pred</th>\n",
              "      <td>0.153272</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          y_real    y_pred\n",
              "y_real  1.000000  0.153272\n",
              "y_pred  0.153272  1.000000"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-jFWC-IUgJSh",
        "outputId": "e522a0fa-5b13-45cc-8a52-6d69933b2a77"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_real</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.940797</td>\n",
              "      <td>0.719177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.846241</td>\n",
              "      <td>0.992057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.955734</td>\n",
              "      <td>0.640758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.868315</td>\n",
              "      <td>0.990559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.978875</td>\n",
              "      <td>0.855068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.964800</td>\n",
              "      <td>0.697250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.715180</td>\n",
              "      <td>0.491895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.905502</td>\n",
              "      <td>0.874358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.960625</td>\n",
              "      <td>1.104769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.982534</td>\n",
              "      <td>0.968954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      y_real    y_pred\n",
              "0   0.940797  0.719177\n",
              "1   0.846241  0.992057\n",
              "2   0.955734  0.640758\n",
              "3   0.868315  0.990559\n",
              "4   0.978875  0.855068\n",
              "..       ...       ...\n",
              "95  0.964800  0.697250\n",
              "96  0.715180  0.491895\n",
              "97  0.905502  0.874358\n",
              "98  0.960625  1.104769\n",
              "99  0.982534  0.968954\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}